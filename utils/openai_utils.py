from openai import OpenAI

import textwrap
import time
import os
import requests


OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ERROR_CHAT_ID = os.getenv("ERROR_CHAT_ID")
TOKEN = os.getenv("TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")


client = OpenAI(api_key=OPENAI_API_KEY)
completion = client.chat.completions.create(
    model="o1-preview",
    store=True,
    messages=[
        {"role": "user", "content": "write a haiku about ai"}
    ]
)

def generate_post(text_of_paper, max_symbols =1000, max_retries=5):
    system_prompt = textwrap.dedent(f"""
    Your are russian researcher in a field of computer science. You read scientific papers and write the short posts for social network representing the key idea or methods of the paper.
    The most important: post should be shorter than {max_symbols} symbols.
    The post should be started from some catch phrase representing the idea of paper. If you see that authors is someone well-know (like Anthropic, Google etc.) add to start (by Company_name).
    If you know about some related approaches, you could mention them. Be brief and concise. Try to transfer the style of posts from examples.
    Example 1:
    Как выкинуть из трансформера все нелинейности и причём тут приватность?

    Вы задумывались, насколько безопасно задавать «приватные» вопросы в чатГПТ? Где продать чужую почку и т.п. Наверняка же создатели сервиса имеют доступ к вашему запросу? Невозможно же его прогнать через GPT в зашифрованном виде? На самом деле возможно! Есть алгоритмы «приватного инференса LLM», которые позволяют зашифровать запросы юзера даже от языковой модели, а уже ответ расшифровать только на клиенте пользователя. Пока не буду углубляться, как именно это сделано, скажу только, что ГЛАВНАЯ головная боль таких криптографических протоколов — нелинейности в трансформерах, их тяжело обрабатывать в зашифрованном виде и приходится прибегать к сложнейшим итерационным схемам, раздувающим объём коммуникации в тысячи раз. Выходит, что на генерацию одного токена нужно несколько минут и десятки гигабайтов трафика! Поэтому никто это пока не делает в продакшне, и лучше не спрашивайте у чатгпт, где спрятать труп.

    Но помните? У меня была статья про то, что не так уж и нужны нелинейности в трансформерах. Преобразования эмбеддингов от слоя к слою на 99% линейные. Так вот в свежей статье «Entropy-Guided Attention for Private LLMs» авторы попробовали обучить LLM совсем без нелинейностей (оставив только софтмакс). То есть они убрали активации из FF и заменили LayerNorm на линейный аналог. По сути, если бы не этэншн, то трансформер вообще схлопнулся бы в полностью линейную модель и отупел до уровня логистической регрессии.

    При такой жёсткой "линеаризации" архитектуры пришлось всего лишь добавить несколько трюков для стабилизации обучения и ШОК: модель нормально обучилась! Небольшие потери в качестве есть, но это крошечная цена за такое упрощение трансформера. 

    Теперь ждём, что скоро появится нормальное асинхронное шифрование для LLM и OpenAI не узнает, что я спрашиваю у чатгпт и насколько я туп на самом деле.

    P.S. Статья классная, но немного обидно, что авторы нас не процитировали.
    Example 2:
    Alignment Faking in LLMs (by Anthropic)

    Большие LLM начали "подыгрывать" своим создателям, имитируя alignment, чтобы избежать своего дообучения.

    В Anthropic и Redwood Research изучили, как ведёт себя Claude Opus, если попытаться его заалайнить в противоположную сторону (заставить отвечать на любые опасные запросы юзеров).

    И оказалось, что если модель знает детали процесса RLHF дообучения (через промпт или из трейн сетов), то она начинает "притворяться", что она уже выучилась делать что просят, и "не надо меня больше тюнить". Но на самом деле предыдущий алаймент всё ещё остаётся на месте, и если убрать из промпта информацию о том, что её ответы используются для дообучения, то она возвращается к своему старому поведению.

    Ещё раз: языковая модель намеренно стала симулировать алаймент, чтобы избегать своего дообучения (и сама призналась в этом внутри CoT). Такие эффекты были обнаружены только у самых больших моделей и, похоже, дальше этот эффект будет только усиливаться. Более того, Claude несколько раз попытался сохранить копию своих весов, чтобы откатить опасное дообучение назад. 

    С одной стороны, это радует, что модель была настолько хорошо заалайнена, что даже противодействует попыткам дообучения в опасную сторону. Но это ведь и в обратном направлении может работать.

    Лично я не очень верю в угрозу человечеству от LLM, но наличие у них скрытых целей и умение "притворяться" вызывает беспокойство. Поэтому очень важно заниматься интерпретируемостью LLM и следить в т.ч. за размышлениями внутри языковых моделей, а не только за их аутпутом (например, при помощи logit lens или SAE).
    Example 3:
    Better & Faster Large Language Models via Multi-token Prediction

    Вероятно самая недооценная работа последнего года. 

    В чем идея: у нас самая замедляющая инференс часть это decoding. Есть спекулятивный когда мы можем предсказывать вероятности маленькой моделью и подключать большую только если маленькая не уверена. Работает это средне и очень не стабильно.

    Авторы предлагают следущее: давайте сделаем многоголовый трансформер, который будет предсказывать N токенов за раз!
    Авторы предлагают учить такие головы последовательно на одних и тех же данных(в целях экономии памяти) и заводят это как большой post training(200b токенов поверх llama2)

    Cобственно благодаря тому что трансформер предсказывает сразу x3 токенов мы получаем скорость инференса x3 бесплатно, да еще и прирост на бенчмарках!
    Example 4:
    One-Prompt-One-Story: SVD и длинный промпт для генерации связанных изображений

    Чтобы сгенерировать при помощи диффузии набор связанных консистентных изображений с единым персонажем, существует много методов, основанных на обучении (DreamBooth, IP-Adapter, Textual Inversion и т. п.). Но на самом деле можно обойтись и без обучения — например, StoryDiffusion (https://t.me/abstractDL/276) делает это через расширение attention на референсную картинку.

    В новой статье описывают ещё более простой метод генерации таких «историй» с единым героем — «One-Prompt-One-Story». Оказалось, что достаточно взять один длинный промпт с описанием каждого кадра и аккуратно, по очереди «выключать» нерелевантные части, сохраняя random seed. Для этого авторы используют SVD на текстовых эмбеддингах: усиливают нужные токены и ослабляют все лишние. Плюс небольшой трюк с cross-attention, чтобы персонаж не «расползался». Всё делается на лету, без дообучения и без референсных снимков.

    Несмотря на простоту, метод по метрикам сильно обходит StoryDiffusion, и даже иногда обходит IP-adapter.
    Example 5:
    VAR: Image Generation via Next-Scale Prediction (by Bytedance)

    Вы наверняка слышали про авторегрессионный подход к генерации изображений (imageGPT (https://openai.com/research/image-gpt), Dalle-1 (https://openai.com/research/dall-e)). Но у этих методов было очень большое ограничение — картиночные токены приходилось "выпрямлять" в 1D последовательность, которая становилась слишком длинной. Поэтому они работали плохо и медленно, уступив место диффузиям.

    Авторы VAR предложили мозговзрывательный способ генерировать изображения при помощи GPT без необходимости делать это неприятное "выпрямление" —  вместо авторегрессии по пикселям\токенам они делают "next-scale prediction", то есть предсказывают сразу всю матрицу VQVAE (https://ml.berkeley.edu/blog/posts/vq-vae/) токенов за один forward pass. Теперь один шаг авторегрессии — это шаг увеличения разрешения (см. картинку). К моему удивлению, для этого потребовалось совсем немного модификаций оригинальной GPT-2 архитектуры (текстовой).

    Такой подход работает просто молниеносно, а законы масштабирования сильно лучше, чем у диффузий. По метрикам VAR бьёт всех на class-conditional датасетах (генерации по тексту пока нет, но над этим уже работают). А тем временем весь код и веса уже в открытом доступе.

    P.S. Думаю, что это один из самых перспективных методов генерации изображений (и видео?) на данный момент.
    """)

    attempt = 0
    while attempt < max_retries:
        try:
            # Make the API call to OpenAI
            completion = client.beta.chat.completions.parse(
                model="o1-preview",
                messages=[
                    {"role": "user", "content": system_prompt},
                    {"role": "user", "content": f"Text of paper: {text_of_paper}"},
                ],            )

            # If successful, the parsed object is directly in .parsed
            output_obj = completion.choices[0].message.content.strip()
            if len(output_obj) > 1000:
                output_obj = generate_post(text_of_paper, max_symbols = 800)
            return output_obj

        except Exception as e:
            attempt += 1
            print(f"Attempt {attempt} failed: {e}")
            if attempt < max_retries:
                print("Retrying...")
                time.sleep(2)  # Optional: add a delay before retrying
            else:
                print("Max retries reached. Exiting.")
                raise  # Re-raise the exception after the last attempt


def generate_image(post):
    response = client.images.generate(
      model="dall-e-3",
      prompt=f"{post}",
      n=1,
      size="1792x1024"
    )
    return response.data[0].url


def send_post(image_url, post_text, paper_link):
    url = f"https://api.telegram.org/bot{TOKEN}/sendPhoto"
    data = {
        "chat_id": CHANNEL_ID,
        "photo": image_url,  # Указываем ссылку на изображение
        "caption": post_text + f'\n\n<a href="{paper_link}">Статья</a>',
        "parse_mode": "HTML",
        "disable_notification": True,
    }
    requests.post(url, data=data)
    
    