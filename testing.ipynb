{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.1' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "ending = \"/papers/2502.01061\"\n",
    "base_url = \"https://arxiv.org/pdf/\"\n",
    "if \"papers\" in ending:\n",
    "    ending = ending.replace(\"/papers/\", \"\")\n",
    "url = os.path.join(base_url, ending)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "doc = fitz.open(\"first_paper.pdf\")\n",
    "for i, page in enumerate(doc):\n",
    "    pix = page.get_pixmap(dpi=150)  # or higher DPI\n",
    "    pix.save(f\"page_{i}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found empty bounding boxes: [(0, 0, 1241, 1754)]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def find_empty_areas(image_path, \n",
    "                     threshold_value=180, \n",
    "                     morph_kernel_size=(5,5), \n",
    "                     min_area=5000):\n",
    "    \"\"\"\n",
    "    Find areas on a page image that do NOT contain text (or are mostly white).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image_path : str\n",
    "        Path to the input page image (ideally a grayscale PNG/JPEG).\n",
    "    threshold_value : int\n",
    "        Threshold for binarization. Adjust if your document is lighter/darker.\n",
    "    morph_kernel_size : (int, int)\n",
    "        Size of the structuring element for morphological operations.\n",
    "        Larger kernel can unify text lines and small text blocks more strongly.\n",
    "    min_area : int\n",
    "        Minimum area (in pixels) for a region to be considered a valid \"empty\" area.\n",
    "        Adjust based on image resolution and how large you want the empty area to be.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    empty_bboxes : list of tuples\n",
    "        List of bounding boxes (x, y, w, h) for each region that appears free of text.\n",
    "    mask_empty : np.ndarray (2D, uint8)\n",
    "        A binary mask (same size as input image) where empty areas are white (255)\n",
    "        and text areas are black (0).\n",
    "    \"\"\"\n",
    "    # 1) Load image in grayscale\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Could not load image: {image_path}\")\n",
    "    \n",
    "    # 2) Binarize (invert so text is white on black if you prefer)\n",
    "    #    For this example, let's keep text as black on white\n",
    "    _, bin_img = cv2.threshold(img, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # 3) Morphological operations to expand/unify text regions\n",
    "    #    If your text is dark on a light background, you want to unify black text.\n",
    "    #    So we invert the image to make text white for dilation:\n",
    "    bin_inverted = 255 - bin_img\n",
    "    \n",
    "    # Increase \"white\" (text) areas so that scattered text is grouped together\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, morph_kernel_size)\n",
    "    dilated = cv2.dilate(bin_inverted, kernel, iterations=1)\n",
    "    \n",
    "    # 4) We'll consider the \"text areas\" as everything that is white in `dilated`.\n",
    "    #    The complement of that are the \"empty\" areas.\n",
    "    #    Create a mask for text regions\n",
    "    text_mask = dilated\n",
    "    \n",
    "    # Invert again to get empty areas in white\n",
    "    # Now: text_mask=255 means text, so empty_mask=255 means empty\n",
    "    empty_mask = 255 - text_mask\n",
    "    \n",
    "    # 5) Find contours in the empty_mask to locate blocks of empty space\n",
    "    contours, _ = cv2.findContours(empty_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    empty_bboxes = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        area = w * h\n",
    "        \n",
    "        # Filter out small or trivial spaces\n",
    "        if area > min_area:\n",
    "            empty_bboxes.append((x, y, w, h))\n",
    "    \n",
    "    return empty_bboxes, empty_mask\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    bboxes, mask_empty = find_empty_areas(\"page_3.png\",\n",
    "                                          threshold_value=180,\n",
    "                                          morph_kernel_size=(10,10),\n",
    "                                          min_area=10000)\n",
    "    \n",
    "    print(\"Found empty bounding boxes:\", bboxes)\n",
    "    # Save the mask for visualization\n",
    "    cv2.imwrite(\"page_0_empty_mask.png\", mask_empty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected bounding boxes for large black regions:\n",
      "  (x,y,w,h)= (327, 419, 30, 198)\n",
      "  (x,y,w,h)= (612, 406, 51, 85)\n",
      "  (x,y,w,h)= (383, 406, 53, 94)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def find_large_black_regions(image_path,\n",
    "                             threshold_value=128,\n",
    "                             morph_kernel_size=(3, 3),\n",
    "                             coverage_threshold=0.5,\n",
    "                             min_area=2000):\n",
    "    \"\"\"\n",
    "    Detect large black regions in a binary image that are likely images/figures,\n",
    "    excluding smaller or sparse text blocks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_path : str\n",
    "        Path to the input document page image (preferably grayscale).\n",
    "    threshold_value : int\n",
    "        Threshold value for binarization (0-255). Adjust as needed.\n",
    "    morph_kernel_size : (int, int)\n",
    "        Kernel size for optional morphological operations to unify large blocks.\n",
    "    coverage_threshold : float\n",
    "        Minimum coverage ratio to consider a region \"large black region.\"\n",
    "        E.g., 0.5 means at least 50% of the bounding box must be black.\n",
    "    min_area : int\n",
    "        Minimum area in pixels of the bounding box to be considered. \n",
    "        Excludes tiny specks (and presumably text lines if very small).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    image_bboxes : list of (x, y, w, h)\n",
    "        Bounding boxes of the detected image-like regions.\n",
    "    mask_images : np.ndarray\n",
    "        A binary mask (same size as the input) where the large black regions are white.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Load the image in grayscale\n",
    "    gray = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if gray is None:\n",
    "        raise FileNotFoundError(f\"Could not load image: {image_path}\")\n",
    "    \n",
    "    # 2) Binarize (ensure black text/figures = 0, white background = 255)\n",
    "    #    Depending on the document, you might invert. But let's assume black on white.\n",
    "    _, bin_img = cv2.threshold(gray, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # OPTIONAL: Morphological closing to unify dark regions\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, morph_kernel_size)\n",
    "    closed = cv2.morphologyEx(bin_img, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "    \n",
    "    # 3) Find contours of dark regions (which are 0 in a 0-255 image).\n",
    "    #    We can invert so contours find white shapes, or we specify RETR_EXTERNAL for black shapes.\n",
    "    #    Often easier to invert so the shapes become white:\n",
    "    inverted = 255 - closed  \n",
    "    contours, _ = cv2.findContours(inverted, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    image_bboxes = []\n",
    "    mask_images = np.zeros_like(bin_img, dtype=np.uint8)  # will mark found regions as white\n",
    "    \n",
    "    # 4) Analyze each contour\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        box_area = w * h\n",
    "        if box_area < min_area:\n",
    "            continue  # ignore very small objects\n",
    "        \n",
    "        # 5) Coverage ratio: how many black pixels are in this contour’s bounding box?\n",
    "        #    Let's create a mask for this contour alone:\n",
    "        contour_mask = np.zeros_like(bin_img, dtype=np.uint8)\n",
    "        cv2.drawContours(contour_mask, [cnt], 0, color=255, thickness=-1)\n",
    "        \n",
    "        #    Count how many black/white pixels are in that bounding box region:\n",
    "        #    In 'inverted', the region is white=255, so we can count it directly in 'contour_mask'\n",
    "        #    or we can just count within the bounding box.\n",
    "        contour_pixels = cv2.countNonZero(contour_mask[y:y+h, x:x+w])\n",
    "        \n",
    "        # coverage ratio = contour_pixels / bounding_box_area\n",
    "        coverage = contour_pixels / float(box_area)\n",
    "        \n",
    "        # 6) If coverage is large enough, treat as an image block\n",
    "        if coverage >= coverage_threshold:\n",
    "            # Mark region in the mask\n",
    "            cv2.drawContours(mask_images, [cnt], 0, color=255, thickness=-1)\n",
    "            image_bboxes.append((x, y, w, h))\n",
    "\n",
    "    return image_bboxes, mask_images\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    path = \"page_3.png\"\n",
    "    bboxes, mask_img = find_large_black_regions(\n",
    "        image_path=path,\n",
    "        threshold_value=128,\n",
    "        morph_kernel_size=(3, 3),\n",
    "        coverage_threshold=0.5,  # 50% coverage\n",
    "        min_area=2000\n",
    "    )\n",
    "    \n",
    "    print(\"Detected bounding boxes for large black regions:\")\n",
    "    for bb in bboxes:\n",
    "        print(\"  (x,y,w,h)=\", bb)\n",
    "    \n",
    "    # Save the mask for visualization\n",
    "    cv2.imwrite(\"large_black_regions_mask.png\", mask_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved crop 0 to figure_0.png\n",
      "Saved crop 1 to figure_1.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def extract_images_from_bboxes(image_path, bboxes, output_prefix=\"crop\"):\n",
    "    \"\"\"\n",
    "    Extract and save image crops from an input image using provided bounding boxes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_path : str\n",
    "        Path to the source image (e.g., a page in PNG/JPG).\n",
    "    bboxes : list of tuple\n",
    "        List of (x, y, w, h) bounding boxes. Coordinates are in pixel space:\n",
    "            x,y -> top-left corner\n",
    "            w -> width\n",
    "            h -> height\n",
    "    output_prefix : str\n",
    "        Prefix for the saved cropped images.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    crops : list of np.ndarray\n",
    "        A list of the cropped image regions (as arrays).\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Could not load image: {image_path}\")\n",
    "    \n",
    "    crops = []\n",
    "\n",
    "    # For each bounding box, crop the region and save\n",
    "    for i, (x, y, w, h) in enumerate(bboxes):\n",
    "        # Ensure coordinates are within image bounds\n",
    "        # (in case the bbox extends off the image edge)\n",
    "        x_end = min(x + w, image.shape[1])\n",
    "        y_end = min(y + h, image.shape[0])\n",
    "\n",
    "        # Crop\n",
    "        crop_img = image[y:y_end, x:x_end]\n",
    "        crops.append(crop_img)\n",
    "\n",
    "        # Save to disk\n",
    "        out_name = f\"{output_prefix}_{i}.png\"\n",
    "        cv2.imwrite(out_name, crop_img)\n",
    "        print(f\"Saved crop {i} to {out_name}\")\n",
    "\n",
    "    return crops\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Suppose you have bounding boxes from some previous analysis\n",
    "    found_bboxes = [\n",
    "        (327, 419, 30, 198),  # (x, y, w, h)\n",
    "        (500, 100, 200, 200)\n",
    "    ]\n",
    "    \n",
    "    # Call the function\n",
    "    cropped_images = extract_images_from_bboxes(\"page_0.png\", found_bboxes, output_prefix=\"figure\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting layoutparser\n",
      "  Downloading layoutparser-0.3.4-py3-none-any.whl.metadata (7.7 kB)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement detectron2==0.6 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for detectron2==0.6\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install layoutparser \"layoutparser[ocr]\" detectron2==0.6 opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module layoutparser has no attribute Detectron2LayoutModel",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     image_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_0.png\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# a page you've converted from PDF\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     boxes, crops \u001b[38;5;241m=\u001b[39m \u001b[43mextract_figures_with_layoutparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected figure bounding boxes:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (x1, y1, x2, y2) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(boxes):\n",
      "Cell \u001b[0;32mIn[14], line 36\u001b[0m, in \u001b[0;36mextract_figures_with_layoutparser\u001b[0;34m(image_path, model_config, score_threshold)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load image file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# 2. Initialize LayoutParser’s Detectron2 model\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#    The label_map for PubLayNet usually is:\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#    0: \"Text\", 1: \"Title\", 2: \"List\", 3: \"Table\", 4: \"Figure\"\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDetectron2LayoutModel\u001b[49m(\n\u001b[1;32m     37\u001b[0m     config_path\u001b[38;5;241m=\u001b[39mmodel_config,\n\u001b[1;32m     38\u001b[0m     extra_config\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODEL.ROI_HEADS.SCORE_THRESH_TEST\u001b[39m\u001b[38;5;124m\"\u001b[39m, score_threshold],\n\u001b[1;32m     39\u001b[0m     label_map\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m0\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mList\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m3\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m4\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFigure\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     40\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# or \"cuda\" if you have a GPU\u001b[39;00m\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 3. Run detection on the image\u001b[39;00m\n\u001b[1;32m     44\u001b[0m layout \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdetect(image)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/layoutparser/file_utils.py:226\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    224\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[0;31mAttributeError\u001b[0m: module layoutparser has no attribute Detectron2LayoutModel"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import layoutparser as lp\n",
    "\n",
    "def extract_figures_with_layoutparser(image_path, \n",
    "                                      model_config=\"lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config\", \n",
    "                                      score_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Detect and extract figures from a document page image using LayoutParser's \n",
    "    Detectron2 model (trained on PubLayNet).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_path : str\n",
    "        Path to the page image (PNG/JPG).\n",
    "    model_config : str\n",
    "        Model config from the LayoutParser model zoo.\n",
    "    score_threshold : float\n",
    "        Confidence threshold for detected layouts.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    figure_bboxes : list of tuple\n",
    "        A list of bounding boxes (x1, y1, x2, y2) for detected figures.\n",
    "    figure_crops : list of np.ndarray\n",
    "        Corresponding cropped image regions for each figure.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Read the page image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Could not load image file: {image_path}\")\n",
    "\n",
    "    # 2. Initialize LayoutParser’s Detectron2 model\n",
    "    #    The label_map for PubLayNet usually is:\n",
    "    #    0: \"Text\", 1: \"Title\", 2: \"List\", 3: \"Table\", 4: \"Figure\"\n",
    "    model = lp.Detectron2LayoutModel(\n",
    "        config_path=model_config,\n",
    "        extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", score_threshold],\n",
    "        label_map={0:\"Text\", 1:\"Title\", 2:\"List\", 3:\"Table\", 4:\"Figure\"},\n",
    "        device=\"cpu\"  # or \"cuda\" if you have a GPU\n",
    "    )\n",
    "\n",
    "    # 3. Run detection on the image\n",
    "    layout = model.detect(image)\n",
    "\n",
    "    # 4. Filter out only 'Figure' blocks\n",
    "    figure_blocks = [b for b in layout if b.type == \"Figure\"]\n",
    "\n",
    "    figure_bboxes = []\n",
    "    figure_crops = []\n",
    "\n",
    "    # 5. Crop out each detected figure\n",
    "    for i, fig_block in enumerate(figure_blocks):\n",
    "        # LayoutParser stores coordinates in a Rect object\n",
    "        x1, y1, x2, y2 = map(int, fig_block.block.coordinates)  # [left, top, right, bottom]\n",
    "\n",
    "        # Ensure valid bounding box within image boundaries\n",
    "        x1 = max(0, x1); y1 = max(0, y1)\n",
    "        x2 = min(image.shape[1], x2)\n",
    "        y2 = min(image.shape[0], y2)\n",
    "\n",
    "        crop_img = image[y1:y2, x1:x2]\n",
    "        figure_bboxes.append((x1, y1, x2, y2))\n",
    "        figure_crops.append(crop_img)\n",
    "    \n",
    "    return figure_bboxes, figure_crops\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    image_file = \"page_0.png\"  # a page you've converted from PDF\n",
    "    boxes, crops = extract_figures_with_layoutparser(image_file, score_threshold=0.5)\n",
    "\n",
    "    print(\"Detected figure bounding boxes:\")\n",
    "    for i, (x1, y1, x2, y2) in enumerate(boxes):\n",
    "        print(f\"Figure {i}: (x1={x1}, y1={y1}, x2={x2}, y2={y2})\")\n",
    "        # Save each cropped figure\n",
    "        out_path = f\"figure_{i}.png\"\n",
    "        cv2.imwrite(out_path, crops[i])\n",
    "        print(f\"  -> saved figure to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
